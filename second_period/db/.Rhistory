} else{
stop("ERRO: OPÇÃO INVÁLIDA")
}
train = get_split(data = data_train, pct_train = 0.75)[[1]]
validation = get_split(data = data_train, pct_train = 0.75)[[2]]
class = validation %>% dplyr::select(churn_label)
validation = get_split(data = data_train, pct_train = 0.75)[[2]] %>%
dplyr::select(-churn_label)
train = get_var_select_manual(db = train)
validation = get_var_select_manual(db = validation)
train = get_impute(data = train)
validation = get_impute(data = validation)
train = get_encoding(encode = "label", data = train)
validation = get_encoding(encode = "label", data = validation)
data_test_raw = get_import(db = "df_test")
data_test = get_var_select_manual(db = data_test_raw)
data_test = get_impute(data = data_test)
data_test = get_encoding(encode = "label", data = data_test)
logit_model <- stats::glm(churn_label ~ ., data = train, family = "binomial")
predictions_prob <- stats::predict(logit_model, newdata = validation, type = "response")
predictions <- ifelse(predictions_prob > THRESHOLD, "Yes", "No")
conf_matrix <- confusionMatrix(factor(predictions, levels = c("Yes", "No")),
factor(class$churn_label, levels = c("Yes", "No")))
print(conf_matrix)
conf_matrix <- table(Predicted = predictions, Actual = class$churn_label)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(accuracy)
precision <- conf_matrix["Yes", "Yes"] / sum(conf_matrix[, "Yes"])
recall <- conf_matrix["Yes", "Yes"] / sum(conf_matrix["Yes", ])
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("Acurácia:", accuracy, "\n")
cat("F1 Score:", f1_score, "\n")
model_rf <- caret::train(churn_label  ~ .,
data = train,
method ='rf',
preProcess = c("center", "scale"),
trControl = trainControl(method ="cv",
number = 10,
allowParallel = TRUE,
# Trees *\
verboseIter = TRUE))
model_tree <- caret::train(churn_label ~ .,
data = train,
method = 'rpart',
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_gbm <- caret::train(churn_label ~ .,
data = train,
method ='gbm',
verbose = FALSE,
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_nn <- caret::train(churn_label ~ .,
data = train,
method = 'nnet',
trace = FALSE,
# preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_lda <- caret::train(churn_label ~ .,
data = train,
method = 'lda',
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_knn <- train(churn_label ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
rf = get_confusionMatrix(model = model_rf, validation) %>% print()
rf_f1 = get_f1_score(cm = rf) %>% print()
tree = get_confusionMatrix(model = model_tree, validation) %>% print()
tree_f1 = get_f1_score(cm = tree) %>% print()
gbm = get_confusionMatrix(model = model_gbm, validation) %>% print()
gbm_f1 = get_f1_score(cm = gbm) %>% print()
nn = get_confusionMatrix(model = model_nn, validation) %>% print()
nn_f1 = get_f1_score(cm = nn) %>% print()
lda = get_confusionMatrix(model = model_lda, validation) %>% print()
lda_f1 = get_f1_score(cm = lda) %>% print()
knn = get_confusionMatrix(model = model_knn, validation) %>% print()
knn_f1 = get_f1_score(cm = knn) %>% print()
model_compare <- data.frame(model = c('Random Forest',
'Trees',
'Gradient Boosting',
"Rede Neural",
'Linear Discriminant',
'KNN',
"Logit"),
accuracy = c(rf$overall[1],
tree$overall[1],
gbm$overall[1],
nn$overall[1],
lda$overall[1],
knn$overall[1],
accuracy),
F1 = c(rf_f1[1],
tree_f1[1],
gbm_f1[1],
nn_f1[1],
lda_f1[1],
knn_f1[1],
f1_score)) %>%
tidyr::pivot_longer(cols = accuracy:F1)
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
theme(legend.position = "bottom")+
theme_minimal()
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
theme(legend.position = "bottom")+
theme(legend.position = "bottom")+
theme_minimal()
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
labs(y = "Métricas", x = "",
title = "Métricas ML")+
theme_minimal()+
theme(legend.position = "bottom")
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
geom_text(aes(label = round(value, 2)),
position = position_dodge(width = 0.9),
vjust = -0.5) + # Ajuste a posição vertical do texto
labs(y = "Métricas", x = "",
title = "Métricas ML", fill = "")+
theme_minimal()+
theme(legend.position = "bottom")
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
geom_text(aes(label = round(value, 2)),
position = position_dodge(width = 0.9),
vjust = -0.5) +
ylim(0,1)+
labs(y = "Métricas", x = "",
title = "Métricas ML", fill = "")+
theme_minimal()+
theme(legend.position = "bottom")
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test))
predictions_prob <- stats::predict(logit_model, newdata = data_test, type = "response")
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test))
View(submit_combined)
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test),
tree = predict(model_tree, newdata = data_test),
gbm = predict(model_gbm, newdata = data_test),
nn = predict(model_nn, newdata = data_test),
lda = predict(model_lda, newdata = data_test),
knn = predict(model_knn, newdata = data_test))
View(submit_combined)
# Função para encontrar a moda de um vetor, retornando "rf" em caso de empate bimodal
find_mode <- function(x) {
# Conta a frequência de cada valor
freq_table <- table(x)
# Encontra o valor mais frequente
max_freq <- max(freq_table)
# Obtém todos os valores com a máxima frequência
modes <- names(freq_table[freq_table == max_freq])
# Se houver empate, retorna o valor da coluna 'rf', caso contrário, retorna a moda
if (length(modes) > 1) {
return(x[which(names(x) == "rf")])
} else {
return(modes)
}
}
# Aplica a função find_mode a cada linha do data.frame
submit_combined$mode <- apply(submit_combined[, -1], 1, find_mode)
View(submit_combined)
submit_combined <- submit_combined %>%
select(CustomerID,mode)
unique(submit_combined$mode)
getwd()
utils::write.csv(submit_combined , "model_combined.csv", row.names = FALSE)
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
geom_text(aes(label = round(value, 2)),
position = position_dodge(width = 0.9),
vjust = -0.5) +
ylim(0,1)+
labs(y = "Métricas", x = "",
title = "Métricas ML", fill = "")+
theme_minimal()+
theme(legend.position = "bottom")
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test),
tree = predict(model_tree, newdata = data_test),
gbm = predict(model_gbm, newdata = data_test),
nn = predict(model_nn, newdata = data_test),
lda = predict(model_lda, newdata = data_test),
knn = predict(model_knn, newdata = data_test))
# Função para encontrar a moda de um vetor, retornando "rf" em caso de empate bimodal
find_mode <- function(x) {
# Conta a frequência de cada valor
freq_table <- table(x)
# Encontra o valor mais frequente
max_freq <- max(freq_table)
# Obtém todos os valores com a máxima frequência
modes <- names(freq_table[freq_table == max_freq])
# Se houver empate, retorna o valor da coluna 'rf', caso contrário, retorna a moda
if (length(modes) > 1) {
return(x[which(names(x) == "rf")])
} else {
return(modes)
}
}
# Aplica a função find_mode a cada linha do data.frame
submit_combined$mode <- apply(submit_combined[, -1], 1, find_mode)
View(submit_combined)
submit_combined <- submit_combined %>%
select(CustomerID,gbm)
getwd()
utils::write.csv(submit_combined , "model_gbm.csv", row.names = FALSE)
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test),
tree = predict(model_tree, newdata = data_test),
gbm = predict(model_gbm, newdata = data_test),
nn = predict(model_nn, newdata = data_test),
lda = predict(model_lda, newdata = data_test),
knn = predict(model_knn, newdata = data_test))
# Função para encontrar a moda de um vetor, retornando "rf" em caso de empate bimodal
find_mode <- function(x) {
# Conta a frequência de cada valor
freq_table <- table(x)
# Encontra o valor mais frequente
max_freq <- max(freq_table)
# Obtém todos os valores com a máxima frequência
modes <- names(freq_table[freq_table == max_freq])
# Se houver empate, retorna o valor da coluna 'rf', caso contrário, retorna a moda
if (length(modes) > 1) {
return(x[which(names(x) == "rf")])
} else {
return(modes)
}
}
# Aplica a função find_mode a cada linha do data.frame
submit_combined$mode <- apply(submit_combined[, -1], 1, find_mode)
submit_combined <- submit_combined %>%
select(CustomerID,rf)
View(submit_combined)
utils::write.csv(submit_combined , "model_rf.csv", row.names = FALSE)
base::set.seed(123)
base::rm(list = ls())
grDevices::graphics.off()
base::options(scipen = 999)
setwd("~/Github/Projetos/IEEE/second_period/db")
pacman::p_load(tidyverse, data.table,
DataExplorer, naniar,
Metrics, vip, dlookr, caret, rsample, glmnet, stats)
THRESHOLD = 0.5
get_import <- function(db){
data_raw = data.table::fread(paste0(db, ".csv")) %>%
janitor::clean_names()
return(data_raw)
}
get_split <- function(data, pct_train){
set.seed(123)
data_split = rsample::initial_split(data = data, prop = pct_train)
train = rsample::training(data_split)
test  = rsample::testing(data_split)
return(list(train, test))
}
get_var_select_manual <- function(db){
data_select = db %>%
dplyr::select(-customer_id:-state,
-city,
-zip_code,
-lat_long,
-paperless_billing,
-payment_method,
-cltv)
return(data_select)
}
get_impute <- function(data){
data_impute = data %>%
dplyr::mutate(across(where(is.numeric), ~ ifelse(is.na(.),
median(., na.rm = TRUE), .)))
return(data_impute)
}
get_encoding <- function(encode, data){
if(encode == "label"){
data_encoded = data %>%
dplyr::mutate(across(where(is.character), as.factor))
}else if(encode == "one_hot"){
dummy_model = caret::dummyVars("~ .", data = data, fullRank = TRUE)
data_encoded = stats::predict(dummy_model,
newdata = data) %>% as.data.frame()
}else{
stop("ERRO: OPÇÃO INVÁLIDA!")
}
return(data_encoded)
}
balance_dataset <- function(data, target_col, seed = 123) {
# Configurar a semente para reprodutibilidade *\
set.seed(seed)
yes_samples <- subset(data, get(target_col) == "Yes")
no_samples <- subset(data, get(target_col) == "No")
# Subamostrar a classe "No" para igualar o número de amostras da classe "Yes"
no_samples_sub = no_samples[sample(nrow(no_samples), nrow(yes_samples)), ]
balanced_data = base::rbind(yes_samples, no_samples_sub)
balanced_data = balanced_data[sample(nrow(balanced_data)), ]
return(balanced_data)
}
get_confusionMatrix <- function(model, validation){
predict_model = stats::predict(model, validation)
cm = caret::confusionMatrix(predict_model,
as.factor(class$churn_label))
return(cm)
}
get_f1_score <- function(cm) {
precision <- cm$byClass["Pos Pred Value"]
recall <- cm$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)
return(f1_score)
}
balance = "yes"
balance = "no"
if(balance == "no"){
data_train = get_import(db = "df_train")
print(table(data_train$churn_label))
} else if(balance == "yes"){
data_train = balance_dataset(get_import(db = "df_train"), target_col = "churn_label")
print(table(data_train$churn_label))
} else{
stop("ERRO: OPÇÃO INVÁLIDA")
}
train = get_split(data = data_train, pct_train = 0.75)[[1]]
validation = get_split(data = data_train, pct_train = 0.75)[[2]]
class = validation %>% dplyr::select(churn_label)
validation = get_split(data = data_train, pct_train = 0.75)[[2]] %>%
dplyr::select(-churn_label)
train = get_var_select_manual(db = train)
validation = get_var_select_manual(db = validation)
train = get_impute(data = train)
validation = get_impute(data = validation)
train = get_encoding(encode = "label", data = train)
validation = get_encoding(encode = "label", data = validation)
data_test_raw = get_import(db = "df_test")
data_test = get_var_select_manual(db = data_test_raw)
data_test = get_impute(data = data_test)
data_test = get_encoding(encode = "label", data = data_test)
logit_model <- stats::glm(churn_label ~ ., data = train, family = "binomial")
predictions_prob <- stats::predict(logit_model, newdata = validation, type = "response")
predictions <- ifelse(predictions_prob > THRESHOLD, "Yes", "No")
conf_matrix <- confusionMatrix(factor(predictions, levels = c("Yes", "No")),
factor(class$churn_label, levels = c("Yes", "No")))
print(conf_matrix)
conf_matrix <- table(Predicted = predictions, Actual = class$churn_label)
print(conf_matrix)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(accuracy)
precision <- conf_matrix["Yes", "Yes"] / sum(conf_matrix[, "Yes"])
recall <- conf_matrix["Yes", "Yes"] / sum(conf_matrix["Yes", ])
f1_score <- 2 * (precision * recall) / (precision + recall)
cat("Acurácia:", accuracy, "\n")
cat("F1 Score:", f1_score, "\n")
predictions_prob <- stats::predict(logit_model, newdata = data_test, type = "response")
submit_prediction = data.frame("CustomerID" = data_test_raw$customer_id,
"Churn Label" = ifelse(predictions_prob > THRESHOLD, "Yes", "No"))
getwd()
model_rf <- caret::train(churn_label  ~ .,
data = train,
method ='rf',
preProcess = c("center", "scale"),
trControl = trainControl(method ="cv",
number = 10,
allowParallel = TRUE,
# Trees *\
verboseIter = TRUE))
model_tree <- caret::train(churn_label ~ .,
data = train,
method = 'rpart',
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_gbm <- caret::train(churn_label ~ .,
data = train,
method ='gbm',
verbose = FALSE,
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_nn <- caret::train(churn_label ~ .,
data = train,
method = 'nnet',
trace = FALSE,
# preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_lda <- caret::train(churn_label ~ .,
data = train,
method = 'lda',
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
model_knn <- train(churn_label ~ .,
data = train,
method = "knn",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv",
number = 10,
allowParallel = TRUE,
verboseIter = TRUE))
rf = get_confusionMatrix(model = model_rf, validation) %>% print()
rf_f1 = get_f1_score(cm = rf) %>% print()
tree = get_confusionMatrix(model = model_tree, validation) %>% print()
rf = get_confusionMatrix(model = model_rf, validation) %>% print()
rf_f1 = get_f1_score(cm = rf) %>% print()
tree = get_confusionMatrix(model = model_tree, validation) %>% print()
tree_f1 = get_f1_score(cm = tree) %>% print()
gbm = get_confusionMatrix(model = model_gbm, validation) %>% print()
gbm_f1 = get_f1_score(cm = gbm) %>% print()
nn = get_confusionMatrix(model = model_nn, validation) %>% print()
nn_f1 = get_f1_score(cm = nn) %>% print()
lda = get_confusionMatrix(model = model_lda, validation) %>% print()
lda_f1 = get_f1_score(cm = lda) %>% print()
knn = get_confusionMatrix(model = model_knn, validation) %>% print()
knn_f1 = get_f1_score(cm = knn) %>% print()
model_compare <- data.frame(model = c('Random Forest',
'Trees',
'Gradient Boosting',
"Rede Neural",
'Linear Discriminant',
'KNN',
"Logit"),
accuracy = c(rf$overall[1],
tree$overall[1],
gbm$overall[1],
nn$overall[1],
lda$overall[1],
knn$overall[1],
accuracy),
F1 = c(rf_f1[1],
tree_f1[1],
gbm_f1[1],
nn_f1[1],
lda_f1[1],
knn_f1[1],
f1_score)) %>%
tidyr::pivot_longer(cols = accuracy:F1)
ggplot(model_compare) +
aes(x = model, y = value, fill = name) +
geom_col(position = "dodge") +
geom_text(aes(label = round(value, 2)),
position = position_dodge(width = 0.9),
vjust = -0.5) +
ylim(0,1)+
labs(y = "Métricas", x = "",
title = "Métricas ML", fill = "")+
theme_minimal()+
theme(legend.position = "bottom")
submit_combined = data.frame("CustomerID" = data_test_raw$customer_id,
Logit = ifelse(predictions_prob > THRESHOLD, "Yes", "No"),
rf = predict(model_rf, newdata = data_test),
tree = predict(model_tree, newdata = data_test),
gbm = predict(model_gbm, newdata = data_test),
nn = predict(model_nn, newdata = data_test),
lda = predict(model_lda, newdata = data_test),
knn = predict(model_knn, newdata = data_test))
# Função para encontrar a moda de um vetor, retornando "rf" em caso de empate bimodal
find_mode <- function(x) {
# Conta a frequência de cada valor
freq_table <- table(x)
# Encontra o valor mais frequente
max_freq <- max(freq_table)
# Obtém todos os valores com a máxima frequência
modes <- names(freq_table[freq_table == max_freq])
# Se houver empate, retorna o valor da coluna 'rf', caso contrário, retorna a moda
if (length(modes) > 1) {
return(x[which(names(x) == "rf")])
} else {
return(modes)
}
}
# Aplica a função find_mode a cada linha do data.frame
submit_combined$mode <- apply(submit_combined[, -1], 1, find_mode)
submit_combined <- submit_combined %>%
select(CustomerID,mode)
View(submit_combined)
utils::write.csv(submit_combined, "model_combined_desbalanceado.csv", row.names = FALSE)
